{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee971e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sdv\n",
      "  Downloading sdv-0.18.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m492.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting graphviz<1,>=0.13.2\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m203.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ctgan<0.8,>=0.7.0\n",
      "  Downloading ctgan-0.7.1-py2.py3-none-any.whl (26 kB)\n",
      "Collecting sdmetrics<0.10,>=0.9.0\n",
      "  Downloading sdmetrics-0.9.2-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<2,>=1.1.3 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from sdv) (1.4.4)\n",
      "Requirement already satisfied: tqdm<5,>=4.15 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from sdv) (4.64.1)\n",
      "Collecting rdt<2,>=1.3.0\n",
      "  Downloading rdt-1.3.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m681.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.20.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from sdv) (1.21.5)\n",
      "Collecting cloudpickle<3.0,>=2.1.0\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting Faker<15,>=10\n",
      "  Downloading Faker-14.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting deepecho<0.5,>=0.4.0\n",
      "  Downloading deepecho-0.4.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting copulas<0.9,>=0.8.0\n",
      "  Downloading copulas-0.8.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m247.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib<4,>=3.4.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from copulas<0.9,>=0.8.0->sdv) (3.5.2)\n",
      "Requirement already satisfied: scipy<2,>=1.5.4 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from copulas<0.9,>=0.8.0->sdv) (1.9.1)\n",
      "Collecting torch<2,>=1.8.0\n",
      "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m679.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging<22,>=20 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from ctgan<0.8,>=0.7.0->sdv) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from Faker<15,>=10->sdv) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from pandas<2,>=1.1.3->sdv) (2022.1)\n",
      "Requirement already satisfied: psutil<6,>=5.7 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from rdt<2,>=1.3.0->sdv) (5.9.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=0.24 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from rdt<2,>=1.3.0->sdv) (1.2.1)\n",
      "Collecting plotly<6,>=5.10.0\n",
      "  Downloading plotly-5.13.1-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->copulas<0.9,>=0.8.0->sdv) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->copulas<0.9,>=0.8.0->sdv) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->copulas<0.9,>=0.8.0->sdv) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->copulas<0.9,>=0.8.0->sdv) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from matplotlib<4,>=3.4.0->copulas<0.9,>=0.8.0->sdv) (9.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from plotly<6,>=5.10.0->sdmetrics<0.10,>=0.9.0->sdv) (8.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.4->Faker<15,>=10->sdv) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from scikit-learn<2,>=0.24->rdt<2,>=1.3.0->sdv) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/donutearth/anaconda3/lib/python3.9/site-packages (from scikit-learn<2,>=0.24->rdt<2,>=1.3.0->sdv) (2.2.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/donutearth/anaconda3/lib/python3.9/site-packages (from torch<2,>=1.8.0->ctgan<0.8,>=0.7.0->sdv) (4.3.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/donutearth/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.8.0->ctgan<0.8,>=0.7.0->sdv) (63.4.1)\n",
      "Requirement already satisfied: wheel in /home/donutearth/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.8.0->ctgan<0.8,>=0.7.0->sdv) (0.37.1)\n",
      "Installing collected packages: plotly, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, graphviz, cloudpickle, nvidia-cudnn-cu11, Faker, torch, rdt, copulas, sdmetrics, deepecho, ctgan, sdv\n",
      "  Attempting uninstall: plotly\n",
      "    Found existing installation: plotly 5.9.0\n",
      "    Uninstalling plotly-5.9.0:\n",
      "      Successfully uninstalled plotly-5.9.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.3.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.3.3 requires pyqtwebengine<5.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Faker-14.2.1 cloudpickle-2.2.1 copulas-0.8.0 ctgan-0.7.1 deepecho-0.4.0 graphviz-0.20.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 plotly-5.13.1 rdt-1.3.0 sdmetrics-0.9.2 sdv-0.18.0 torch-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464e95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.cluster as cluster\n",
    "from numpy import array, random, sum, unique\n",
    "from pandas import DataFrame, read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05337731",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20585/2831485551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import imblearn\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from kneed import KneeLocator\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a279fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.metrics import binary_accuracy\n",
    "from tensorflow.keras.layers import Convolution1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import RMSprop, Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3811422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTMs\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e33d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the dataset for ML\n",
    "# Text file data converted to integer data type\n",
    "import numpy as np\n",
    "x_data = np.loadtxt(\"dataset/x_dataTrain.txt\", dtype=float)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf05294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the dataset for ML\n",
    "# Text file data converted to integer data type\n",
    "y_data = np.loadtxt(\"dataset/y_dataTrain.txt\", dtype=float)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42648274",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features['timestep'], features[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637024ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying PCA for random forest regression\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "principalComponents = pca.fit_transform(scaled_features)\n",
    "#principalDf = pd.DataFrame(data = principalComponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_values = np.arange(10) + 1\n",
    "PC_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing scree plot\n",
    "PC_values = np.arange(10) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_[:10], 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=7)\n",
    "principalComponents = pca.fit_transform(scaled_features)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['PC1','PC2','PC3','PC4', 'PC5', 'PC6', 'PC7'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c568f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(principalDf.iloc[78,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a76f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    0 : 'H2O',\n",
    "    1 : 'CO2',\n",
    "    2 : 'O2',\n",
    "    3 : 'N2',\n",
    "    4 : 'CH4',\n",
    "    5 : 'N2O',\n",
    "    6 : 'CO',\n",
    "    7 : 'O3',\n",
    "    8 : 'SO2',\n",
    "    9 : 'NH3',\n",
    "    10 : 'C2H6',\n",
    "    11 : 'NO2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee816458",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(y_data)\n",
    "labels.rename(columns=feature_dict,\n",
    "          inplace=True)\n",
    "labelsSum = labels[['H2O','CO2', 'O2', 'N2', 'CH4']]\n",
    "\n",
    "labelsSum['H2O'] = labelsSum['H2O'].apply(lambda x: x*100)\n",
    "labelsSum['CO2'] = labelsSum['CO2'].apply(lambda x: x*100)\n",
    "labelsSum['O2'] = labelsSum['O2'].apply(lambda x: x*100)\n",
    "labelsSum['N2'] = labelsSum['N2'].apply(lambda x: x*100)\n",
    "labelsSum['CH4'] = labelsSum['CH4'].apply(lambda x: x*100)\n",
    "\n",
    "labelsSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cee963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#principalDf['H2O'] = labelsSum['H2O']\n",
    "principalDf['CO2'] = labelsSum['CO2']\n",
    "#principalDf['O2'] = labelsSum['O2']\n",
    "#principalDf['N2'] = labelsSum['N2']\n",
    "#principalDf['CH4'] = labelsSum['CH4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d856ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create synthetic data\n",
    "from sdv.tabular import GaussianCopula\n",
    "model = GaussianCopula()\n",
    "model.fit(principalDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.sample(num_rows=10003)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.hist(column='CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987648cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf.hist(column='CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metrics.tabular import CSTest, KSTest\n",
    "KSTest.compute(principalDf, sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metrics.tabular import LogisticDetection, SVCDetection\n",
    "print(LogisticDetection.compute(principalDf, sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15254ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metrics.tabular import MLPRegressor\n",
    "MLPRegressor.compute(principalDf, sample, target='CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0324781",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = principalDf.sample(int(len(principalDf) * 0.75))\n",
    "test = principalDf[~principalDf.index.isin(train.index)]\n",
    "MLPRegressor.compute(train, test, target='CO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c27dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synth_xData = model.sample(num_rows=100003)\n",
    "synth_xData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_xData.hist(column=\"CO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080063cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf.hist(column=\"CO2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = synth_xData.iloc[:,:-1]\n",
    "y_label =  synth_xData.iloc[:,-1:]\n",
    "y_label = y_data[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9adf67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daaf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_x_data = pca.inverse_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9bbbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_xData = pd.DataFrame(synth_x_data)\n",
    "fake_xData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfe58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "test = scaler.fit_transform(fake_xData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d23fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(testFeatures.iloc[5,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dabb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures2 = pd.DataFrame(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffe78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(testFeatures2.iloc[1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(features.iloc[0,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label.iloc[9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = labels.iloc[:,1:2]\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, y_label, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaledTrain = scaler.fit_transform(X_train)\n",
    "X_scaledTest = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f9cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_scaledTrain = y_scaler.fit_transform(y_train)\n",
    "y_scaledTest = y_scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0782577",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaledTrain = np.squeeze(X_scaledTrain)\n",
    "X_scaledTest = np.squeeze(X_scaledTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaledTrain = np.expand_dims(X_scaledTrain,2)\n",
    "X_scaledTest = np.expand_dims(X_scaledTest,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaledTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546aa481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='tanh', input_shape=(X_scaledTrain.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Lambda(lambda x: K.dropout(x,level=0.05)))\n",
    "    model.add(Dense(y_scaledTrain.shape[-1], activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f21a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 10000\n",
    "lengthscale = .01\n",
    "reg = lengthscale**2 * (1 - 0.05) / (2. * 100000 * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904dc9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='tanh', input_shape=(X_scaledTrain.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer = regularizers.l2(reg)))\n",
    "    model.add(Lambda(lambda x: K.dropout(x,level=0.05)))\n",
    "    model.add(Dense(y_scaledTrain.shape[-1], activation='relu'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Neural Network model\n",
    "def network():\n",
    "    exoAtmos = Sequential()\n",
    "\n",
    "    exoAtmos.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    exoAtmos.add(BatchNormalization())\n",
    "    exoAtmos.add(Dropout(0.05))\n",
    "    \n",
    "    exoAtmos.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    exoAtmos.add(BatchNormalization())\n",
    "    exoAtmos.add(Dropout(0.05))\n",
    "\n",
    "\n",
    "    # The Output Layer :\n",
    "    exoAtmos.add(Dense(1))\n",
    "\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=1)\n",
    "    # Compile the network :\n",
    "    exoAtmos.compile(loss='mae', optimizer=opt)\n",
    "    \n",
    "    return exoAtmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d6063",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn = network()\n",
    "sgd = tensorflow.keras.optimizers.SGD(lr=0.1)\n",
    "cnn.compile(loss='mean_squared_error')#,optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4960052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a09f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = cnn.fit(X_scaledTrain, y_scaledTrain,\n",
    "                          epochs=10,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9340a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93aa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "mae = cnn.evaluate(X_scaledTest, y_scaledTest, verbose=0)\n",
    "print('>%.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab712f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "exoPredict = cnn.predict(X_scaledTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a1a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exoPred_df = pd.DataFrame(exoPredict, columns = [0])\n",
    "ypred_df = pd.DataFrame(y_scaledTest, columns = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49535c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exoPred_df.hist(column=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b26630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred_df.hist(column=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
